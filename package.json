{
  "name": "robots-txt-parser",
  "version": "1.0.3",
  "description": "A lightweight robots.txt parser for Node.js with support for wildcards, caching and promises.",
  "keywords": [
    "robots",
    "txt",
    "robots.txt",
    "parser",
    "crawler",
    "spider",
    "bot",
    "robotstxt",
    "scraper"
  ],
  "main": "src/index.js",
  "directories": {
    "test": "test"
  },
  "scripts": {
    "test": "mocha test/**/*.js",
    "test-robots": "mocha test/robots/*.js",
    "test-parser": "mocha test/parser/*.js",
    "test-util": "mocha test/util/*.js",
    "coverage": "nyc mocha test/**/*.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/ChrisAkroyd/robots-txt-parser.git"
  },
  "author": "Chris Akroyd",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/ChrisAkroyd/robots-txt-parser/issues"
  },
  "homepage": "https://github.com/ChrisAkroyd/robots-txt-parser#readme",
  "dependencies": {
    "fast-url-parser": "^1.1.3",
    "is-absolute-url": "^2.1.0",
    "simple-get": "^2.4.0"
  },
  "devDependencies": {
    "chai": "^4.3.4",
    "lodash": "^4.17.21",
    "mocha": "^8.3.2",
    "nyc": "^15.1.0"
  },
  "files": [
    "LICENSE",
    "README.md",
    "src/*.js"
  ]
}
